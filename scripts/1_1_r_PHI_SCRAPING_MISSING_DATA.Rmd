---
title: "PHI enriching the dataset by scraping the missing data"
author: "Petra Hermankova"
date: "27/05/2020"
output: rmarkdown::github_document

---
*Setting up the environment*
```{r setup, echo=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE)
devtools::install_github("mplex/cedhar", subdir="pkg/sdam")

library(tidyverse)
library(purrr)
library(tidytext)
library(dplyr)
library(sdam)
library(jsonlite)
library(formatR)
library(xml2)
library(rvest)
library(stringr)
library(reshape2)
```

1. Load the credentials from external txt file
```{r}
mycred_secret<- readLines("~/mysecret.txt")
```

2. Make the request 
```{r, echo = TRUE }
resp = request("PHI_merged_2020-08-24.json", path="/sharingin/648597@au.dk/SDAM_root/SDAM_data/PHI/", method="GET", cred=mycred_secret)

# remove any traces of the secret password from the document
remove(mycred_secret)
```

3. Make a list from the request and display the first six records (head)
```{r}
list_json <- jsonlite::fromJSON(resp, flatten=TRUE)
PHI_tibble = as_tibble(list_json)
PHI_unboxed_json <- jsonlite::toJSON(PHI_tibble, auto_unbox=TRUE) 
list_unboxed_json <- jsonlite::fromJSON(PHI_unboxed_json)
PHI_tibble = as_tibble(list_unboxed_json)
PHI_tibble
```

# Filtering only the records that have missing data (contain only word Regions) in the hdr1 column
```{r}
regions <- PHI_tibble %>% 
  filter(hdr1 == "Regions") %>% 
  select('PHI_ID')
```


# Scenario 1: Scrape missing data (where hdr1=="Regions")

Source: https://towardsdatascience.com/coupling-web-scraping-with-functional-programming-in-r-for-scale-1bc4509eef29

```{r}
baseurl <- "https://epigraphy.packhum.org/text/"

baseurl %>% 
  map2_chr(regions_vector, paste0) %>% 
  map(. %>% 
        xml2::read_html() %>% 
        rvest::html_node("body") %>% 
        rvest::html_children() %>% 
        rvest::html_children() %>% 
        rvest::html_children() %>% 
        xml2::xml_find_all("//div[contains(@class, 'hdr1')]") %>% 
        rvest::html_text() %>% 
        stringr::str_trim() ) %>% 
  unlist() -> hdr1


regions_all <- data.frame(hdr1, regions)
regions_all %>% 
  rename("PHI_ID" = regions) -> regions_all
regions_all


# unlist regions_all
regions_all_df <- unnest(regions_all, c("hdr1", "PHI_ID"))
```

Write out as CSV (in case computer crashes)
```{r}
write_csv(regions_all_df, "./regions_all_2020-08-24.csv")
```

# Scenario 2: If you have already scraped Regions in CSV: 

Read in from CSV
```{r}
regions_all_df <- read_csv("./regions_all_2020-08-24.csv")
```

Showing unique values
```{r}
unique(regions_all_df$hdr1)
```

Cleaning strings for consistency, sometimes Regions with the following \n and sometimes without
```{r}
str_replace(regions_all_df$hdr1, "Regions", "Regions\n") -> regions_all_df$hdr1
```

Writing the newly scraped hdr1 to the original dataset and checking the unique values, no column containing only Regions exists
```{r}
if (regions_all_df$PHI_ID %in% PHI_tibble$PHI_ID){
  str_replace_all(string = PHI_tibble$hdr1, pattern = "Regions$", replacement = regions_all_df$hdr1) %>% 
  print()
} -> PHI_tibble$hdr1

if (regions_all_df$PHI_ID %in% PHI_tibble$PHI_ID){
  str_replace_all(string = PHI_tibble$hdr1, pattern = "NULL", replacement = regions_all_df$hdr1) %>% 
  print()
} -> PHI_tibble$hdr1

unique(PHI_tibble$hdr1)
```

# Saving to Sciencedata
```{r Saving to Sciencedata}
PHI_tibble = as_tibble(PHI_tibble)

PHI_df <- as.data.frame(PHI_tibble)

PHI_json <- jsonlite::toJSON(PHI_df, auto_unbox=TRUE) 
write(PHI_json, file="PHI_enriched_2020-12-16.json")
mycred_secret<- readLines("~/mysecret.txt")
request("PHI_enriched_2020-12-16.json", path="/sharingout/648597@au.dk/SDAM_root/SDAM_data/PHI/",
        method="PUT", cred=c(mycred_secret[1], mycred_secret[2]))
#remove(mycred_secret)
```


```{r}
 file.remove("./PHI_enriched_2020-12-16.json") # removes local copy
```


